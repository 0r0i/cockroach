* Create a new DB type in kv/ which uses a storage.DB implementation,
  either one of DistDB or LocalDB. The kv.DB deals with coordinator,
  intercepts, etc.

* Encode basic key value types:
  - Counter
  - Bag o' bytes
  - Error if increment is used on a bag o' bytes
  - Error if Put/Get used on a counter
  - Should Scan return counters?

* Construct a base handler for all HTTP servers to glog.Fatal the
  process with a deferred recover func to prevent HTTP from swallowing
  panics which might otherwise be holding locks, etc.

* Add Request/Response interfaces to avoid reflection in getting/setting
  errors or fetching the Request/Response headers.

* Transactions

  - Generate random priority, pick candidate timestamp.

  - Modified ops:

    - Get: logic to support clock skew uncertainty and concurrency.

      Simple, common case: if most recent timestamp for key is less
      than Timestamp and committed, read value.

      If most recent timestamp for key is greater than MaxTimestamp
      (it can be either committed or an intent), and there are no
      versions of key between Timestamp and MaxTimestamp, read value
      for key at Timestamp.

      If there are version(s) of the key (can include most recent
      intent) with timestamp between Timestamp and MaxTimestamp,
      return WriteWithinUncertaintyInterval error. The ResponseHeader
      will contain the latest version's timestamp (actually, just the
      latest version with timestamp <= MaxTimestamp); on transaction
      retry, Timestamp will be min(key's timestamp + 1, MaxTimestamp),
      and MaxTimestamp will be max(key's timestamp + 1, MaxTimestamp).

      In the event an intent is encountered with timestamp <=
      Timestamp, try to push the transaction to Timestamp + 1. If
      already committed, resolve and retry the read. If push succeeds,
      read value for key at Timestamp. If push fails, backoff and
      retry the transaction.

      After reading any value, update the read-timestamp-cache with
      the txn's Timestamp.

    - Put: additions to support intent marker.

      If entry exists but is intent:

        - If intent owned by another txn, try to push transaction
          (with "Abort"=true). If result of push is already-committed
          or aborted, resolve the existing intent and retry put. If
          push succeeds (meaning the other txn is aborted), delete
          existing intent and write new intent. If push fails, backoff
          and retry the transaction.

    New operations on transaction table rows:

    - PushTransaction: Moves transaction timestamp forwards.

      If existing txn entry isn't present or its LastHeartbeat
      timestamp isn't set, use PushTxn.Timestamp as LastHeartbeat.
      If current time - LastHeartbeat > MaxHeartbeatExpiry, then
      the existing txn should be either pushed forward or aborted,
      depending on value of Request.Abort.

      If the txn is committed, return already-committed error. If txn
      has been aborted, noop and return success.

      Otherwise, Compare PushTxn and Txn priorities:

        - If Txn.Priority < PushTxn.Priority, return retry-txn
          error. Transaction will be retried with priority =
          max(random, PushTxn.Priority - 1).

        - If Txn.Priority > PushTxn.Priority, set/add txn entry with
          new timestamp as max(existing timestamp, push timestamp + 1).
          If Request.Abort is true, set/add ABORTED txn entry.

      type PushTransactionRequest struct {
        RequestHeader
        Key     Key         // derivative of txn table prefix & txn ID
        Abort   bool        // abort txn on successful push--this is done for puts
        PushTxn Transaction // from encounted txn intent
        Txn     Transaction // txn which encountered intent
      }

      type PushTransactionResponse struct {
        ResponseHeader
      }


* StoreFinder using Gossip protocol to filter


* Range split

  - Split criteria: is range larger than max_range_bytes?

  - Transactionally rewrite range addressing indexes.

    - Need a range-wide "split" intent which blocks all mutating
      ops to range.

  - Copy / split range-local metadata:

    - Write new local range metadata

    - Copy response cache

* Rebalance range replica. Only fully-replicated ranges may be
  rebalanced.

  - Keep a rebalance queue in memory. Range replicas are added to the
    queue from a store during initial range scan and also during
    operation as a response to certain conditions. Listed here:

    - A range is split. Each replica in the split range is marked as
      needing rebalancing.

    - Replica not matching zone config. When zone config changes happen,
      all ranges are scanned by each store and any mismatched replicas
      are added to the queue.

  - Rebalance away from stores finding themselves in top N space
    utilized, taking care to account for fewer than N stores in
    cluster. Only stores finding themselves in the top N space
    utilized set may have rebalances in effect.

  - Rebalance target is selected from available stores in bottom N
    space utilized. Adjacent stores are exempted.

  - Add rebalance target to replica set and rewrite range addressing
    indexes.

  - Rebalance targets are added to replica set always exactly one at a
    time. Targets are marked as REBALANCING. Obsolete sources are
    marked as PENDING_DELETION. Any time a range becomes fully
    replicated, the range leader replica will move REBALANCING
    replicas into state OK and will remove PENDING_DELETION replicas
    from the RangeDescriptor. The store which owns a removed replica
    is responsible for clearing the relevant portion of the key space
    as well as any other housekeeping details.

* Move mvcc + engine files out of storage and into mvcc/
